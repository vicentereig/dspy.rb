# Stop Wrestling with Prompts: How DSPy.rb Brings Software Engineering to LLM Development

You're staring at a prompt template for the third hour today. You tweak a word, test it, watch it fail mysteriously on edge cases, then start over. Sound familiar? If you're building with LLMs, you've been there. The traditional approach to prompt engineering feels like writing code with string concatenationâ€”it works until it spectacularly doesn't.

Meet DSPy.rb, a Ruby port of Stanford's DSPy framework that transforms how we build with language models. Instead of crafting fragile prompts, you define what you want as typed signatures and let the framework handle the messy details. The result? LLM applications that actually scale and don't break when you sneeze.

## Why DSPy.rb Exists: A Rails Developer's Dilemma

When I first discovered Stanford's DSPy framework, I knew it was the future of LLM development. The programmatic approach, automatic optimization, and modular design solved every pain point I'd experienced with traditional prompting. There was just one problem: I was building Rails applications, not Python scripts.

I faced two choices: rewrite my entire Rails application in Python to use DSPy, or surgically port DSPy's core concepts to Ruby. The first option meant abandoning years of Rails expertise, established patterns, and a thriving ecosystem. The second meant taking on the challenge of bringing Stanford's research to the Ruby world.

That's how DSPy.rb was bornâ€”not as an academic exercise, but as a practical solution for Rails developers who needed DSPy's power without abandoning Ruby. The result is a framework that preserves DSPy's revolutionary approach while embracing Ruby idioms and integrating seamlessly with Rails applications.

## The Prompt Engineering Nightmare

Let's be honest about traditional prompting. You start simple:

```ruby
prompt = "Classify the sentiment of this text: #{text}"
response = openai_client.chat(messages: [{ role: "user", content: prompt }])
# Hope the response is what you expect... ðŸ¤ž
```

Then reality hits. You need confidence scores. The model occasionally returns "Positive sentiment detected" instead of just "positive". Sometimes it adds explanations you don't want. You patch with more instructions:

```ruby
prompt = <<~PROMPT
  Classify the sentiment of the following text as exactly one of: positive, negative, neutral
  Only return the sentiment, nothing else.
  Text: #{text}
  
  Important: Return only the sentiment word, no explanations or formatting.
PROMPT
```

But it still breaks. You add examples, more rules, validation logic. Before you know it, you're maintaining 200-line prompt templates that nobody wants to touch. Testing becomes a nightmare. Versioning? Good luck.

This is string concatenation thinking applied to AI. We can do better.

## Enter Signatures: Define What You Want, Not How to Ask

DSPy.rb flips this approach. Instead of crafting prompts, you define typed signatures that describe your task:

```ruby
require 'dspy'

class ClassifySentiment < DSPy::Signature
  description "Classify the sentiment of a given text"

  class Sentiment < T::Enum
    enums do
      Positive = new('positive')
      Negative = new('negative') 
      Neutral = new('neutral')
    end
  end

  input do
    const :text, String
  end

  output do
    const :sentiment, Sentiment
    const :confidence, Float
  end
end

# Configure your LLM
DSPy.configure do |c|
  c.lm = DSPy::LM.new('openai/gpt-4o-mini', api_key: ENV['OPENAI_API_KEY'])
end

# Create and use the predictor
classifier = DSPy::Predict.new(ClassifySentiment)
result = classifier.call(text: "This Ruby gem is fantastic!")

puts result.sentiment    # => #<Sentiment::Positive>
puts result.confidence   # => 0.89
```

What just happened? You defined what you want (sentiment classification with confidence) using Ruby types. DSPy.rb generated the appropriate prompt, handled the LLM communication, parsed the response, and returned properly typed objects. No string wrangling, no parsing headaches.

The signature acts as a contract. The framework knows how to prompt the model, what format to expect, and how to validate responses. When the model returns "Very positive!" instead of "positive", DSPy.rb handles the conversion automatically.

## Modularity: Building Blocks That Actually Compose

Traditional prompting doesn't compose well. Combine two prompts and you get a mess. DSPy.rb is differentâ€”every component is a module you can reuse and combine.

Need step-by-step reasoning? Wrap your signature with ChainOfThought:

```ruby
class SolveMathProblem < DSPy::Signature
  description "Solve a math word problem"
  
  input do
    const :problem, String
  end
  
  output do
    const :answer, String
  end
end

# Chain of thought automatically adds reasoning
math_solver = DSPy::ChainOfThought.new(SolveMathProblem)
result = math_solver.call(problem: "If I buy 3 apples at $0.50 each and 2 oranges at $0.75 each, how much do I spend?")

puts result.reasoning  # => "Let me solve this step by step..."
puts result.answer     # => "$3.00"
```

Need an agent that can use tools? Use ReAct:

```ruby
class SearchAndAnswer < DSPy::Signature
  description "Search for information and provide an answer"
  
  input do
    const :question, String
  end
  
  output do
    const :answer, String
  end
end

# ReAct adds tool-using capabilities
agent = DSPy::ReAct.new(SearchAndAnswer, tools: [search_tool, calculator_tool])
result = agent.call(question: "What's the population of Tokyo in 2024?")
```

For complex agentic workflows where your agent needs to choose between different action types dynamically, DSPy.rb's union types provide elegant solutions:

```ruby
# Define different action types
class SearchAction < T::Struct
  const :query, String
  const :source, String
end

class CalculateAction < T::Struct  
  const :expression, String
end

class AnswerAction < T::Struct
  const :response, String
end

# Agent chooses action type automatically
class AgentDecision < DSPy::Signature
  description "Choose the appropriate action for the user's request"
  
  input do
    const :request, String
  end
  
  output do
    const :action, T.any(SearchAction, CalculateAction, AnswerAction)
  end
end

agent = DSPy::Predict.new(AgentDecision)
result = agent.call(request: "What's 15% of 250?")
# => result.action will be properly typed as CalculateAction
```

Learn more about this pattern in: [Union Types: The Secret to Cleaner AI Agent Workflows](https://vicentereig.github.io/dspy.rb/blog/union-types-agentic-workflows/).

Want to dive deeper into building ReAct agents? Check out this step-by-step tutorial that walks through creating a research agent with multiple tools: [Building Smart Agents with DSPy.rb: A ReAct Tutorial](https://vicentereig.github.io/dspy.rb/blog/articles/react-agent-tutorial/).

These modules compose naturally. Want a math-solving agent with step-by-step reasoning? Combine them:

```ruby
class MathAgent < DSPy::Module
  def initialize
    @solver = DSPy::ReAct.new(
      DSPy::ChainOfThought.new(SolveMathProblem), 
      tools: [calculator_tool, search_tool]
    )
  end
  
  def call(problem:)
    @solver.call(problem: problem)
  end
end
```

Each module handles one concern well. The result is LLM applications built from composable, testable pieces instead of monolithic prompt scripts.

## The Magic: Automatic Prompt Optimization

Here's where DSPy.rb gets interesting. Remember those hours tweaking prompts? The framework can do that automatically, and better than you can manually.

Meet MIPROv2, the prompt optimizer that actually works:

```ruby
# Your signature (same as before)
class ClassifySentiment < DSPy::Signature
  # ... definition ...
end

# Create some training examples
training_examples = [
  DSPy::Example.new(text: "I love this product!", sentiment: Sentiment::Positive),
  DSPy::Example.new(text: "Worst purchase ever", sentiment: Sentiment::Negative),
  # ... more examples
]

# Set up optimization
program = DSPy::Predict.new(ClassifySentiment)
metric = proc { |example, prediction| prediction.sentiment == example.expected_sentiment }
optimizer = DSPy::Teleprompt::MIPROv2.new(metric: metric)

# Let it optimize automatically
result = optimizer.compile(program, trainset: training_examples)

# Get your optimized predictor
best_predictor = result.optimized_program
puts "Improved from #{result.baseline_score} to #{result.best_score_value}"
puts "Optimized instruction: #{best_predictor.prompt.instruction}"
```

MIPROv2 runs a three-phase optimization:

1. **Bootstrap Phase**: Generates high-quality training examples with reasoning traces
2. **Instruction Phase**: Tries different instruction phrasings and selects the best
3. **Few-shot Phase**: Finds the optimal combination of examples to include

The results are impressive. In testing, MIPROv2 consistently improves performance by 10-30% over manual prompts. More importantly, it finds optimizations humans missâ€”better phrasings, effective examples, and prompt structures that work reliably.

DSPy.rb includes a comprehensive evaluation framework that goes beyond simple accuracy metrics. You can create custom metrics, track multiple performance dimensions, and get detailed evaluation reports. Learn more about building robust evaluation pipelines: [Evaluation Framework Guide](https://vicentereig.github.io/dspy.rb/optimization/evaluation/).

## Production-Ready From Day One

DSPy.rb isn't just for experimentation. It's built for production:

**Type Safety**: Sorbet integration means type errors at compile time, not runtime. No more discovering your LLM returned unexpected formats in production.

**Observability**: Built-in OpenTelemetry support gives you traces, metrics, and logs out of the box:

```ruby
DSPy.configure do |c|
  c.lm = DSPy::LM.new('openai/gpt-4o-mini', api_key: ENV['OPENAI_API_KEY'])
  c.instrumentation.langfuse.api_key = ENV['LANGFUSE_API_KEY']  # Automatic tracing
end
```

**Error Handling**: Automatic retries with exponential backoff. Strategy fallbacks when structured outputs fail. No more mysterious production failures.

**Token Tracking**: Automatic cost monitoring with detailed usage analytics:

```ruby
result = classifier.call(text: "Some text")
puts result.usage.total_tokens      # => 150
puts result.usage.input_tokens      # => 75
puts result.usage.output_tokens     # => 75
```

**Storage & Versioning**: Save optimized programs and reload them later:

```ruby
# Save optimized program
storage = DSPy::ProgramStorage.new(path: "models/sentiment_classifier_v2.json")
storage.save(best_predictor, metadata: { version: "2.1", accuracy: 0.94 })

# Load in production
loaded_predictor = storage.load("models/sentiment_classifier_v2.json")
```

**Advanced Memory Systems**: DSPy.rb includes sophisticated memory capabilities for building stateful applications:

```ruby
# Create a memory system with embeddings
memory = DSPy::Memory.new(
  store: DSPy::InMemoryStore.new,
  embedding_engine: DSPy::LocalEmbeddingEngine.new
)

# Store conversational context
memory.store("user_123", "Customer prefers technical explanations", 
             metadata: { type: "preference", timestamp: Time.now })

# Retrieve relevant memories
relevant = memory.retrieve("user_123", "How should I explain this API error?")
# => Returns contextually relevant memories based on semantic similarity
```

Memory systems enable building chatbots that remember context, recommendation engines that learn preferences, and agents that accumulate knowledge over time. The system supports automatic compaction, deduplication, and smart retrieval strategies. Learn more: [Memory Systems Guide](https://vicentereig.github.io/dspy.rb/advanced/memory-systems/).

## Ruby-Specific Advantages

DSPy.rb isn't just a Python portâ€”it's a Ruby-first framework that leverages Ruby's strengths:

**Native Rails Integration**: DSPy modules work beautifully in Rails controllers and jobs:

```ruby
class EmailClassificationJob < ApplicationJob
  def perform(email_id)
    email = Email.find(email_id)
    
    result = @classifier.call(
      subject: email.subject,
      body: email.body
    )
    
    email.update!(
      category: result.category.serialize,
      priority: result.priority.serialize,
      confidence: result.confidence
    )
  end
end
```

**Ruby Idioms**: Method chaining, blocks, and Ruby's expressive syntax make complex workflows readable:

```ruby
pipeline = DSPy::Pipeline.new
  .add(EmailExtractor.new)
  .add(DSPy::ChainOfThought.new(ClassifyEmail))
  .add(PriorityAssigner.new)

results = emails.map { |email| pipeline.call(email: email) }
```

**Metaprogramming**: Dynamic signature generation based on your models:

```ruby
class DynamicClassifier
  def self.for_enum(enum_class)
    Class.new(DSPy::Signature) do
      description "Classify text into #{enum_class.name} categories"
      
      input { const :text, String }
      output { const :category, enum_class }
    end
  end
end

# Generate classifiers from your enums
sentiment_classifier = DSPy::Predict.new(
  DynamicClassifier.for_enum(SentimentEnum)
)
```

## Real-World Example: Customer Support Email Classifier

Let's build something practicalâ€”an email classifier for customer support that routes tickets automatically.

First, define the types we need:

```ruby
class Email < T::Struct
  const :subject, String
  const :from, String
  const :body, String
end

class Category < T::Enum
  enums do
    Technical = new('technical')
    Billing = new('billing') 
    General = new('general')
  end
end

class Priority < T::Enum
  enums do
    Low = new('low')
    Medium = new('medium')
    High = new('high')
  end
end
```

Now the signature:

```ruby
class ClassifyEmail < DSPy::Signature
  description "Classify customer support emails by category and priority"
  
  input do
    const :email, Email
  end
  
  output do
    const :category, Category
    const :priority, Priority
    const :summary, String, description: "One-line summary for dashboard"
  end
end
```

Set up the complete pipeline:

```ruby
class EmailProcessor < DSPy::Module
  def initialize
    # Use Chain of Thought for better reasoning
    @classifier = DSPy::ChainOfThought.new(ClassifyEmail)
  end
  
  def call(email:)
    @classifier.call(email: email)
  end
end

# Configure
DSPy.configure do |c|
  c.lm = DSPy::LM.new('openai/gpt-4o-mini', api_key: ENV['OPENAI_API_KEY'])
  c.instrumentation.langfuse.api_key = ENV['LANGFUSE_API_KEY']
end

processor = EmailProcessor.new
```

Train and optimize:

```ruby
# Create training data
training_examples = [
  DSPy::Example.new(
    email: Email.new(
      subject: "API returning 500 errors",
      from: "dev@company.com",
      body: "Our production API started returning 500s after the deploy..."
    ),
    category: Category::Technical,
    priority: Priority::High,
    summary: "Production API returning 500 errors after deploy"
  ),
  # ... more examples
]

# Optimize the classifier
program = DSPy::Predict.new(ClassifyEmail)
metric = proc { |example, prediction| prediction.category == example.expected_category }
optimizer = DSPy::Teleprompt::MIPROv2::AutoMode.medium(metric: metric)  # balanced -> medium
result = optimizer.compile(program, trainset: training_examples)
# Note: metric is now used directly by MIPROv2
  evaluator.evaluate(examples: examples) { |ex| predictor.call(email: ex.email) }.score
end

puts "Accuracy improved from #{result.baseline_score} to #{result.best_score_value}"
```

Use in production:

```ruby
email = Email.new(
  subject: "URGENT: Payment failed",
  from: "customer@business.com", 
  body: "My payment failed and I need this resolved ASAP for our launch tomorrow"
)

result = optimized_processor.call(email: email)

puts result.category  # => Category::Billing
puts result.priority  # => Priority::High  
puts result.summary   # => "Payment failure blocking customer launch"
puts result.reasoning # => "The email mentions payment failure and urgency..."

# Route automatically
TicketRouter.route(
  email: email,
  category: result.category,
  priority: result.priority,
  summary: result.summary
)
```

This classifier handles the complexity automaticallyâ€”parsing email structure, understanding urgency indicators, and providing consistent categorization. In testing, MIPROv2 optimization improved accuracy from 73% to 89% over a manually crafted prompt.

For more advanced optimization examples, including medical LLM training with cost analysis and performance tracking, see my detailed case study: [Training Medical LLM Predictors: Process, Costs, and Optimization with DSPy.rb](https://vicente.services/blog/2025/08/11/training-medical-llm-predictors-process,-costs,-and-optimization-with-dspy.rb/).

## Getting Started

Ready to try DSPy.rb? Getting started takes less than 5 minutes:

**Installation:**
```bash
gem install dspy
```

**Basic Setup:**
```ruby
require 'dspy'

# Configure your provider
DSPy.configure do |c|
  # OpenAI (recommended for structured outputs)
  c.lm = DSPy::LM.new('openai/gpt-4o-mini', api_key: ENV['OPENAI_API_KEY'])
  
  # Or Anthropic
  # c.lm = DSPy::LM.new('anthropic/claude-3-5-sonnet-20241022', api_key: ENV['ANTHROPIC_API_KEY'])
  
  # Or local with Ollama
  # c.lm = DSPy::LM.new('ollama/llama3.2')
end
```

**Your First Working Example:**
```ruby
class SimpleQA < DSPy::Signature
  description "Answer questions accurately and concisely"
  
  input { const :question, String }
  output { const :answer, String }
end

qa = DSPy::ChainOfThought.new(SimpleQA)
result = qa.call(question: "What's the capital of France?")

puts result.answer     # => "Paris"
puts result.reasoning  # => "France is a country in Europe, and its capital city is Paris..."
```

That's it. You're building with LLMs using actual software engineering practices.

## The Path Forward

DSPy.rb is actively developed with over 6,800 downloads and regular releases. The roadmap includes exciting developments:

- **Advanced Optimizers**: SIMBA, BootstrapFinetune, and other Stanford DSPy algorithms
- **Enhanced Observability**: Deeper OpenTelemetry integration for production monitoring  
- **Native Reasoning Models**: First-class support for OpenAI's o1 and similar reasoning models
- **Token Efficiency**: BAML-inspired strategies to reduce costs by 50-70%

The framework is production-ready today, with companies using it for customer support, content generation, data extraction, and complex reasoning tasks.

## Stop Wrestling, Start Building

Traditional prompting treats LLMs like magic boxes you whisper to and hope for the best. DSPy.rb treats them like the powerful computing primitives they areâ€”tools that integrate into solid software engineering practices.

With typed signatures, automatic optimization, and production-ready features, DSPy.rb lets you focus on building great applications instead of debugging prompt strings. The 20 hours you'd spend tweaking prompts? Use them to ship features instead.

The Ruby ecosystem now has a serious framework for LLM development. Give DSPy.rb a try on your next projectâ€”your future self will thank you.

**Get started today:**
- Documentation: [vicentereig.github.io/dspy.rb](https://vicentereig.github.io/dspy.rb)
- Gem: `gem install dspy`

*DSPy.rb is an idiomatic Ruby port of Stanford's DSPy framework, adapted for Ruby developers and enhanced with production-ready features. It's MIT licensed and actively maintained.*